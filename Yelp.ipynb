{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "\n",
    "import json\n",
    "\n",
    "from keras.layers import Embedding, LSTM, Dense, Conv1D, MaxPooling1D, Dropout, Activation\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:09.517536\n"
     ]
    }
   ],
   "source": [
    "# Load the reviews and parse JSON\n",
    "t1 = datetime.now()\n",
    "with open(\"chunk00000.json\", encoding='utf-8') as f:\n",
    "    reviews = f.read().strip().split(\"\\n\")\n",
    "reviews = [json.loads(review) for review in reviews]\n",
    "print(datetime.now() - t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a balanced sample of positive and negative reviews\n",
    "texts = [review['text'] for review in reviews]\n",
    "\n",
    "# Convert our 5 classes into 2 (negative or positive)\n",
    "binstars = [0 if review['stars'] <= 3 else 1 for review in reviews]\n",
    "balanced_texts = []\n",
    "balanced_labels = []\n",
    "limit = 100000  # Change this to grow/shrink the dataset\n",
    "neg_pos_counts = [0, 0]\n",
    "for i in range(len(texts)):\n",
    "    polarity = binstars[i]\n",
    "    if neg_pos_counts[polarity] < limit:\n",
    "        balanced_texts.append(texts[i])\n",
    "        balanced_labels.append(binstars[i])\n",
    "        neg_pos_counts[polarity] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({0: 100000, 1: 100000})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(balanced_labels)\n",
    "# >>> Counter({0: 100000, 1: 100000})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=5)\n",
    "toytexts = [\"Is is a common word\", \"So is the\", \"the is common\", \"discombobulation is not common\"]\n",
    "tokenizer.fit_on_texts(toytexts)\n",
    "sequences = tokenizer.texts_to_sequences(toytexts)\n",
    "\n",
    "\n",
    "# >>> [[1, 1, 4, 2], [1, 3], [3, 1, 2], [1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 1, 4, 2], [1, 3], [3, 1, 2], [1, 2]]\n"
     ]
    }
   ],
   "source": [
    "print(sequences)\n",
    "# >>> [[1, 1, 4, 2], [1, 3], [3, 1, 2], [1, 2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'discombobulation': 7, 'so': 6, 'word': 5, 'the': 3, 'common': 2, 'not': 8, 'is': 1, 'a': 4}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 4 2]\n",
      " [0 0 1 3]\n",
      " [0 3 1 2]\n",
      " [0 0 1 2]]\n"
     ]
    }
   ],
   "source": [
    "padded_sequences = pad_sequences(sequences)\n",
    "\n",
    "print(padded_sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(balanced_texts)\n",
    "sequences = tokenizer.texts_to_sequences(balanced_texts)\n",
    "data = pad_sequences(sequences, maxlen=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(20000, 128, input_length=300))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv1D(64, 5, activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=4))\n",
    "model.add(LSTM(128))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "model.fit(data, np.array(balanced_labels), validation_split=0.5, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(data, np.array(balanced_labels), validation_split=0.5, epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "t1 = datetime.now()\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1,2), min_df=3)\n",
    "classifier = LinearSVC()\n",
    "Xs = vectorizer.fit_transform(balanced_texts)\n",
    "\n",
    "print(datetime.now() - t1)\n",
    "print(Xs.shape)\n",
    "\n",
    "score = cross_val_score(classifier, Xs, balanced_labels, cv=2, n_jobs=-1)\n",
    "\n",
    "print(datetime.now() - t1)\n",
    "print(score)\n",
    "print(sum(score) / len(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# save the tokenizer and model\n",
    "with open(\"keras_tokenizer.pickle\", \"wb\") as f:\n",
    "   pickle.dump(tokenizer, f)\n",
    "model.save(\"yelp_sentiment_model.hdf5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
